{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-2-49a2efa78bc6>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fmnist = input_data.read_data_sets('data/fashion', source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def LeakyReLU(x, leak=0.2, name='LeakyReLU'):\n",
    "    with tf.variable_scope(name):\n",
    "        f1 = 0.5 * (1 + leak)\n",
    "        f2 = 0.5 * (1 - leak)\n",
    "        return f1 * x + f2 * abs(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fully_connected_layer(x, output_size, initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    w = tf.get_variable(name + '_weight', [x.get_shape()[1], output_size], initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', [output_size], initializer=initializer, dtype=tf.float32)\n",
    "    \n",
    "    l = tf.nn.bias_add(tf.matmul(x, w), b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization)\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv_2d(x, kernel_size, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = int(x.get_shape()[-1])\n",
    "\n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-1], initializer=initializer)\n",
    "    c = tf.nn.conv2d(x, w, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization, name=name + '_layer_batch_norm')\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def deconv_2d(x, kernel_size, output_shape, stride_size=[1, 1, 1, 1], padding='SAME', initializer=tf.truncated_normal_initializer(stddev=1e-2), activation=tf.nn.relu, batch_normalization=None, name=''):\n",
    "    if type(kernel_size) == tuple: kernel_size = list(kernel_size)\n",
    "    if kernel_size[2] == -1: kernel_size[2] = output_shape[-1]\n",
    "    if kernel_size[3] == -1: kernel_size[3] = int(x.get_shape()[-1])\n",
    "    \n",
    "    if type(output_shape) == tuple: output_shape = list(output_shape)\n",
    "    if output_shape[0] == -1: output_shape[0] = tf.shape(x)[0]\n",
    "    \n",
    "    w = tf.get_variable(name + '_weight', kernel_size, initializer=initializer)\n",
    "    b = tf.get_variable(name + '_bias', kernel_size[-2], initializer=initializer)\n",
    "    c = tf.nn.conv2d_transpose(x, w, output_shape=output_shape, strides=stride_size, padding=padding)\n",
    "    \n",
    "    l = tf.nn.bias_add(c, b, name=name + '_layer')\n",
    "    \n",
    "    if batch_normalization != None:\n",
    "        l = tf.layers.batch_normalization(l, **batch_normalization)\n",
    "    \n",
    "    return activation(l, name=name + '_layer_' + activation.__name__), l, w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ModelMaker(object):\n",
    "    def __init__(self, layers_shape):\n",
    "        self.layers_shape = layers_shape\n",
    "        \n",
    "    def __call__(self, x, name, dropout_list=None, reuse=False):\n",
    "        parameters = set()\n",
    "        layers = set()\n",
    "        \n",
    "        last_layer = x\n",
    "        \n",
    "        dropout = None\n",
    "                \n",
    "        # scope set\n",
    "        with tf.variable_scope(name, reuse=reuse) as scope:\n",
    "            # create layers\n",
    "            for i, (layer_type, *layer_shape) in enumerate(self.layers_shape):\n",
    "                \n",
    "                '''\n",
    "                create matching layer\n",
    "                \n",
    "                c2l  : Convolutional 2 Dimention Layer\n",
    "                dc2l : Deconvolutional 2 Dimention Layer\n",
    "                fcl  : Fully Connected Layer(Dense) Layer\n",
    "                mpl  : Max Pooling Layer\n",
    "                rs   : Reshape\n",
    "                flat : Flatten\n",
    "                '''\n",
    "                if layer_type == 'c2l': # Convolutional 2D Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = conv_2d(x=last_layer, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'dc2l': # Deconvolutional 2D Layer\n",
    "                    kernel_shape, output_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = deconv_2d(x=last_layer, output_shape=output_shape, \\\n",
    "                                            kernel_size=kernel_shape, stride_size=stride_shape, \\\n",
    "                                            name=str(i) + '_c2', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'fcl': # Fully Connected Layer\n",
    "                    output_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer, l, w, b = fully_connected_layer(x=last_layer, \\\n",
    "                                                output_size=output_shape, name=str(i) + '_fc', **params)\n",
    "                    \n",
    "                    parameters.add(w)\n",
    "                    parameters.add(b)\n",
    "                    layers.add(last_layer)\n",
    "                    layers.add(l)\n",
    "                    \n",
    "                elif layer_type == 'mpl': # Max Pooling Layer\n",
    "                    kernel_shape, stride_shape, dropout, params = layer_shape\n",
    "                    \n",
    "                    last_layer = tf.nn.max_pool(input=x, ksize=kernel_shape, strides=stride_shape, \\\n",
    "                                    name=str(i) + '_mp_layer', **parmas)\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'rs': # Reshape Layer\n",
    "                    reshape = layer_shape[0]\n",
    "                    last_layer = tf.reshape(last_layer, reshape, name=str(i) + '_reshape')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                elif layer_type == 'flat': # Flat\n",
    "                    try:\n",
    "                        flat_size = int(np.prod(last_layer.get_shape()[1:]))\n",
    "                    except:\n",
    "                        flat_size = tf.reduce_prod(tf.shape(last_layer)[1:])\n",
    "                        \n",
    "                    last_layer = tf.reshape(last_layer, (-1, flat_size), name=str(i) + '_flat')\n",
    "                    \n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                # Dropout Layer\n",
    "                if type(dropout) == int: # var is index\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout_list[dropout], name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                elif type(dropout) == float: # var is constant value\n",
    "                    last_layer = tf.nn.dropout(last_layer, dropout, name=str(i) + '_dropout')\n",
    "                    layers.add(last_layer)\n",
    "                    \n",
    "                \n",
    "                # initialize vars\n",
    "                layer_shape = \\\n",
    "                kernel_shape = \\\n",
    "                stride_shape = \\\n",
    "                dropout = \\\n",
    "                params = None\n",
    "                    \n",
    "            return last_layer, layers, parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "def ArrayToImage(arr):\n",
    "    # 1-dimention array to 2-dimention\n",
    "    size = np.sqrt(arr.shape[0]).astype(int)\n",
    "    arr = arr.reshape(size, size)\n",
    "    \n",
    "    # array to image\n",
    "    img = Image.fromarray(np.uint8(arr))\n",
    "    return img\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "import time\n",
    "\n",
    "# image list display function for 'Jupytor notebook'\n",
    "def DisplayHorizontal(images, header=None, width=\"100%\", figsize=(20, 20), fontsize=20, depth=1):\n",
    "    num_images = len(images)\n",
    "    \n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(num_images):\n",
    "        image = images[i]\n",
    "        \n",
    "        fig.add_subplot(depth, num_images/depth, i+1)\n",
    "        plt.axis('off')\n",
    "        if header != None:\n",
    "            plt.title(header[i], fontsize=fontsize)\n",
    "        plt.imshow(image, cmap='Greys_r', interpolation='nearest')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelBinarizer\n",
    "def OneHotEncoder(label_size):\n",
    "    lb = LabelBinarizer()\n",
    "    lb.fit(range(label_size))\n",
    "    return lb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "label_size = 10 # mnist [0-9]\n",
    "image_size = 784 # mnist [28, 28]\n",
    "image_width = 28\n",
    "image_height = 28\n",
    "image_depth = 1\n",
    "\n",
    "# z: latent random variable\n",
    "z_var = 100\n",
    "z_category = label_size\n",
    "z_weight = 2\n",
    "z = z_var + z_category + z_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "generator_layer_shape=(    \n",
    "    ('fcl', 7 * 7 * 64, None, {'batch_normalization': {'momentum': 0.9, 'epsilon': 1e-4}, 'activation': tf.nn.relu}),\n",
    "    ('rs', (-1, 7, 7, 64),),\n",
    "    ('dc2l', (3, 3, -1, -1), (-1, 14, 14, 32), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': tf.nn.relu}),\n",
    "    ('dc2l', (3, 3, -1, -1), (-1, 28, 28, 1), (1, 2, 2, 1), None, {'padding': 'SAME', 'activation': tf.nn.tanh}),\n",
    ")\n",
    "\n",
    "discriminator_layer_shape=(\n",
    "    ('c2l', (3, 3, -1, 32), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (3, 3, -1, 64), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('c2l', (3, 3, -1, 128), (1, 2, 2, 1), None, {'batch_normalization': {}, 'padding': 'SAME', 'activation': LeakyReLU}),\n",
    "    ('rs', (-1, 4 * 4 * 128),),\n",
    "    #('fcl', 1, None, {'activation': tf.nn.sigmoid})\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 1e-3\n",
    "beta1=5e-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "G_Maker = ModelMaker(generator_layer_shape)\n",
    "D_Maker = ModelMaker(discriminator_layer_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Latent Random Variable\n",
    "    Z_var = tf.placeholder(tf.float32, [None, z_var])\n",
    "    # Data Label Information\n",
    "    Z_category = tf.placeholder(tf.float32, [None, z_category])\n",
    "    # Data Font Information \n",
    "    Z_weight = tf.placeholder(tf.float32, [None, z_weight])\n",
    "\n",
    "    # Concatenate All Variable & Info\n",
    "    # Generator Input / For Fake Data\n",
    "    Z = tf.concat([Z_var, Z_category, Z_weight], axis=1)\n",
    "\n",
    "    # Generator\n",
    "    X_Fake, G_Layers, G_Params = G_Maker(Z, name='generator')\n",
    "\n",
    "    # For Real Data\n",
    "    X_Real = tf.placeholder(tf.float32, [None, image_size])\n",
    "    X_Real_ = tf.reshape(X_Real, [-1, image_width, image_height, image_depth])\n",
    "\n",
    "    Y = tf.placeholder(tf.float32, shape=[None, z_category])\n",
    "    \n",
    "    # Discriminator for Fake Data\n",
    "    FC_Fake, D_Fake_Layers, D_Params = D_Maker(X_Fake, name='discriminator')\n",
    "    # Discriminator for Real Data\n",
    "    FC_Real, D_Real_Layers, _ = D_Maker(X_Real_, name='discriminator', reuse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    # Fake Outputs\n",
    "    with tf.variable_scope('discriminator'):\n",
    "        Fake, Fake_Logits, *_ = fully_connected_layer(FC_Fake, 1, activation=tf.nn.sigmoid, name='prob')\n",
    "        Fake_Category, *_ = fully_connected_layer(FC_Fake, z_category, activation=LeakyReLU, name='label')\n",
    "        Fake_Weight, *_ = fully_connected_layer(FC_Fake, z_weight, activation=tf.nn.tanh, name='weight')\n",
    "\n",
    "    # Real Outputs\n",
    "    with tf.variable_scope('discriminator', reuse=True):\n",
    "        Real, Real_Logits, *_ = fully_connected_layer(FC_Real, 1, activation=tf.nn.sigmoid, name='prob')\n",
    "        Real_Category, *_ = fully_connected_layer(FC_Real, z_category, activation=LeakyReLU, name='label')\n",
    "        Real_Weight, *_ = fully_connected_layer(FC_Real, z_weight, activation=tf.nn.tanh, name='weight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'set' object has no attribute 'op'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-2ba100514ac8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     14\u001b[0m     \u001b[0mLoss_G_Total\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLoss_G\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLoss_G_Category\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mLoss_G_Weight\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 16\u001b[1;33m     \u001b[0mTrain_D\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer_D\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoss_D_Total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mD_Params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     17\u001b[0m     \u001b[0mTrain_G\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mOptimizer_G\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mLoss_G_Total\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mG_Params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mminimize\u001b[1;34m(self, loss, global_step, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, name, grad_loss)\u001b[0m\n\u001b[0;32m    398\u001b[0m         \u001b[0maggregation_method\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maggregation_method\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    399\u001b[0m         \u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcolocate_gradients_with_ops\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 400\u001b[1;33m         grad_loss=grad_loss)\n\u001b[0m\u001b[0;32m    401\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    402\u001b[0m     \u001b[0mvars_with_grad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mv\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mg\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mgrads_and_vars\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mg\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36mcompute_gradients\u001b[1;34m(self, loss, var_list, gate_gradients, aggregation_method, colocate_gradients_with_ops, grad_loss)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_STREAMING_MODEL_PORTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m     \u001b[0mprocessors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to optimize.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    504\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_collection\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mGraphKeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_STREAMING_MODEL_PORTS\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    505\u001b[0m     \u001b[1;31m# pylint: enable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 506\u001b[1;33m     \u001b[0mprocessors\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0m_get_processor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    507\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    508\u001b[0m       \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"No variables to optimize.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\training\\optimizer.py\u001b[0m in \u001b[0;36m_get_processor\u001b[1;34m(v)\u001b[0m\n\u001b[0;32m    201\u001b[0m     \u001b[1;31m# True if and only if `v` was initialized eagerly.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_DenseResourceVariableProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtype\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"VarHandleOp\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0m_DenseResourceVariableProcessor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvariables\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVariable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'set' object has no attribute 'op'"
     ]
    }
   ],
   "source": [
    "with tf.device('/gpu:0'):\n",
    "    Optimizer_D = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "    Optimizer_G = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=beta1)\n",
    "\n",
    "    Loss_D = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Real_Logits, labels=tf.ones_like(Real)) + \\\n",
    "                            tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.zeros_like(Fake)))\n",
    "    Loss_G = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=Fake_Logits, labels=tf.ones_like(Fake)))\n",
    "\n",
    "    Loss_D_Category = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=Y, logits=Real_Category))\n",
    "    Loss_G_Category = tf.reduce_mean(tf.losses.softmax_cross_entropy(onehot_labels=Z_category, logits=Fake_Category))\n",
    "\n",
    "    Loss_G_Weight = tf.losses.mean_squared_error(Fake_Weight, Z_weight)\n",
    "\n",
    "    Loss_D_Total = Loss_D + Loss_D_Category\n",
    "    Loss_G_Total = Loss_G + Loss_G_Category + Loss_G_Weight\n",
    "    \n",
    "    Train_D = Optimizer_D.minimize(Loss_D_Total, var_list=D_Params)\n",
    "    Train_G = Optimizer_G.minimize(Loss_G_Total, var_list=G_Params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
