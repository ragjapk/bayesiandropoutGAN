{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import datetime\n",
    "import random\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-15-49a2efa78bc6>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion\\train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/fashion\\train-labels-idx1-ubyte.gz\n",
      "Extracting data/fashion\\t10k-images-idx3-ubyte.gz\n",
      "Extracting data/fashion\\t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\ragja\\Anaconda3\\envs\\test\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "fmnist = input_data.read_data_sets('data/fashion', source_url='http://fashion-mnist.s3-website.eu-central-1.amazonaws.com/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generator(z, output_channel_dim, training):\n",
    "    with tf.variable_scope(\"generator\", reuse= not training):\n",
    "        \n",
    "        # 8x8x1024\n",
    "        fully_connected = tf.layers.dense(z, 8*8*1024)\n",
    "        fully_connected = tf.reshape(fully_connected, (-1, 8, 8, 1024))\n",
    "        fully_connected = tf.nn.leaky_relu(fully_connected)\n",
    "\n",
    "        # 8x8x1024 -> 16x16x512\n",
    "        trans_conv1 = tf.layers.conv2d_transpose(inputs=fully_connected,\n",
    "                                                 filters=512,\n",
    "                                                 kernel_size=[5,5],\n",
    "                                                 strides=[2,2],\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                                 name=\"trans_conv1\")\n",
    "        batch_trans_conv1 = tf.layers.batch_normalization(inputs = trans_conv1,\n",
    "                                                          training=training,\n",
    "                                                          epsilon=EPSILON,\n",
    "                                                          name=\"batch_trans_conv1\")\n",
    "        trans_conv1_out = tf.nn.leaky_relu(batch_trans_conv1,\n",
    "                                           name=\"trans_conv1_out\")\n",
    "        \n",
    "        # 16x16x512 -> 32x32x256\n",
    "        trans_conv2 = tf.layers.conv2d_transpose(inputs=trans_conv1_out,\n",
    "                                                 filters=256,\n",
    "                                                 kernel_size=[5,5],\n",
    "                                                 strides=[2,2],\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                                 name=\"trans_conv2\")\n",
    "        batch_trans_conv2 = tf.layers.batch_normalization(inputs = trans_conv2,\n",
    "                                                          training=training,\n",
    "                                                          epsilon=EPSILON,\n",
    "                                                          name=\"batch_trans_conv2\")\n",
    "        trans_conv2_out = tf.nn.leaky_relu(batch_trans_conv2,\n",
    "                                           name=\"trans_conv2_out\")\n",
    "        \n",
    "        # 32x32x256 -> 64x64x128\n",
    "        trans_conv3 = tf.layers.conv2d_transpose(inputs=trans_conv2_out,\n",
    "                                                 filters=128,\n",
    "                                                 kernel_size=[5,5],\n",
    "                                                 strides=[2,2],\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                                 name=\"trans_conv3\")\n",
    "        batch_trans_conv3 = tf.layers.batch_normalization(inputs = trans_conv3,\n",
    "                                                          training=training,\n",
    "                                                          epsilon=EPSILON,\n",
    "                                                          name=\"batch_trans_conv3\")\n",
    "        trans_conv3_out = tf.nn.leaky_relu(batch_trans_conv3,\n",
    "                                           name=\"trans_conv3_out\")\n",
    "        \n",
    "        # 64x64x128 -> 128x128x64\n",
    "        trans_conv4 = tf.layers.conv2d_transpose(inputs=trans_conv3_out,\n",
    "                                                 filters=64,\n",
    "                                                 kernel_size=[5,5],\n",
    "                                                 strides=[2,2],\n",
    "                                                 padding=\"SAME\",\n",
    "                                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                                 name=\"trans_conv4\")\n",
    "        batch_trans_conv4 = tf.layers.batch_normalization(inputs = trans_conv4,\n",
    "                                                          training=training,\n",
    "                                                          epsilon=EPSILON,\n",
    "                                                          name=\"batch_trans_conv4\")\n",
    "        trans_conv4_out = tf.nn.leaky_relu(batch_trans_conv4,\n",
    "                                           name=\"trans_conv4_out\")\n",
    "        \n",
    "        # 128x128x64 -> 128x128x3\n",
    "        logits = tf.layers.conv2d_transpose(inputs=trans_conv4_out,\n",
    "                                            filters=3,\n",
    "                                            kernel_size=[5,5],\n",
    "                                            strides=[1,1],\n",
    "                                            padding=\"SAME\",\n",
    "                                            kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                            name=\"logits\")\n",
    "        out = tf.tanh(logits, name=\"out\")\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def discriminator(x, reuse):\n",
    "    with tf.variable_scope(\"discriminator\", reuse=reuse): \n",
    "        \n",
    "        # 128*128*3 -> 64x64x64 \n",
    "        conv1 = tf.layers.conv2d(inputs=x,\n",
    "                                 filters=64,\n",
    "                                 kernel_size=[5,5],\n",
    "                                 strides=[2,2],\n",
    "                                 padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                 name='conv1')\n",
    "        batch_norm1 = tf.layers.batch_normalization(conv1,\n",
    "                                                    training=True,\n",
    "                                                    epsilon=EPSILON,\n",
    "                                                    name='batch_norm1')\n",
    "        conv1_out = tf.nn.leaky_relu(batch_norm1,\n",
    "                                     name=\"conv1_out\")\n",
    "        \n",
    "        # 64x64x64-> 32x32x128 \n",
    "        conv2 = tf.layers.conv2d(inputs=conv1_out,\n",
    "                                 filters=128,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 strides=[2, 2],\n",
    "                                 padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                 name='conv2')\n",
    "        batch_norm2 = tf.layers.batch_normalization(conv2,\n",
    "                                                    training=True,\n",
    "                                                    epsilon=EPSILON,\n",
    "                                                    name='batch_norm2')\n",
    "        conv2_out = tf.nn.leaky_relu(batch_norm2,\n",
    "                                     name=\"conv2_out\")\n",
    "        \n",
    "        # 32x32x128 -> 16x16x256  \n",
    "        conv3 = tf.layers.conv2d(inputs=conv2_out,\n",
    "                                 filters=256,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 strides=[2, 2],\n",
    "                                 padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                 name='conv3')\n",
    "        batch_norm3 = tf.layers.batch_normalization(conv3,\n",
    "                                                    training=True,\n",
    "                                                    epsilon=EPSILON,\n",
    "                                                    name='batch_norm3')\n",
    "        conv3_out = tf.nn.leaky_relu(batch_norm3,\n",
    "                                     name=\"conv3_out\")\n",
    "        \n",
    "        # 16x16x256 -> 16x16x512\n",
    "        conv4 = tf.layers.conv2d(inputs=conv3_out,\n",
    "                                 filters=512,\n",
    "                                 kernel_size=[5, 5],\n",
    "                                 strides=[1, 1],\n",
    "                                 padding=\"SAME\",\n",
    "                                 kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                 name='conv4')\n",
    "        batch_norm4 = tf.layers.batch_normalization(conv4,\n",
    "                                                    training=True,\n",
    "                                                    epsilon=EPSILON,\n",
    "                                                    name='batch_norm4')\n",
    "        conv4_out = tf.nn.leaky_relu(batch_norm4,\n",
    "                                     name=\"conv4_out\")\n",
    "        \n",
    "        # 16x16x512 -> 8x8x1024\n",
    "        conv5 = tf.layers.conv2d(inputs=conv4_out,\n",
    "                                filters=1024,\n",
    "                                kernel_size=[5, 5],\n",
    "                                strides=[2, 2],\n",
    "                                padding=\"SAME\",\n",
    "                                kernel_initializer=tf.truncated_normal_initializer(stddev=WEIGHT_INIT_STDDEV),\n",
    "                                name='conv5')\n",
    "        batch_norm5 = tf.layers.batch_normalization(conv5,\n",
    "                                                    training=True,\n",
    "                                                    epsilon=EPSILON,\n",
    "                                                    name='batch_norm5')\n",
    "        conv5_out = tf.nn.leaky_relu(batch_norm5,\n",
    "                                     name=\"conv5_out\")\n",
    "\n",
    "        flatten = tf.reshape(conv5_out, (-1, 8*8*1024))\n",
    "        logits = tf.layers.dense(inputs=flatten,\n",
    "                                 units=1,\n",
    "                                 activation=None)\n",
    "        out = tf.sigmoid(logits)\n",
    "        return out, logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_loss(input_real, input_z, output_channel_dim):\n",
    "    g_model = generator(input_z, output_channel_dim, True)\n",
    "\n",
    "    noisy_input_real = input_real + tf.random_normal(shape=tf.shape(input_real),\n",
    "                                                     mean=0.0,\n",
    "                                                     stddev=random.uniform(0.0, 0.1),\n",
    "                                                     dtype=tf.float32)\n",
    "    \n",
    "    d_model_real, d_logits_real = discriminator(noisy_input_real, reuse=False)\n",
    "    d_model_fake, d_logits_fake = discriminator(g_model, reuse=True)\n",
    "    \n",
    "    d_loss_real = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_real,\n",
    "                                                                         labels=tf.ones_like(d_model_real)*random.uniform(0.9, 1.0)))\n",
    "    d_loss_fake = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                         labels=tf.zeros_like(d_model_fake)))\n",
    "    d_loss = tf.reduce_mean(0.5 * (d_loss_real + d_loss_fake))\n",
    "    g_loss = tf.reduce_mean(tf.nn.sigmoid_cross_entropy_with_logits(logits=d_logits_fake,\n",
    "                                                                    labels=tf.ones_like(d_model_fake)))\n",
    "    return d_loss, g_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_optimizers(d_loss, g_loss):\n",
    "    t_vars = tf.trainable_variables()\n",
    "    g_vars = [var for var in t_vars if var.name.startswith(\"generator\")]\n",
    "    d_vars = [var for var in t_vars if var.name.startswith(\"discriminator\")]\n",
    "    \n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    gen_updates = [op for op in update_ops if op.name.startswith('generator')]\n",
    "    \n",
    "    with tf.control_dependencies(gen_updates):\n",
    "        d_train_opt = tf.train.AdamOptimizer(learning_rate=LR_D, beta1=BETA1).minimize(d_loss, var_list=d_vars)\n",
    "        g_train_opt = tf.train.AdamOptimizer(learning_rate=LR_G, beta1=BETA1).minimize(g_loss, var_list=g_vars)  \n",
    "    return d_train_opt, g_train_opt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def model_inputs(real_dim, z_dim):\n",
    "    inputs_real = tf.placeholder(tf.float32, (None, *real_dim), name='inputs_real')\n",
    "    inputs_z = tf.placeholder(tf.float32, (None, z_dim), name=\"input_z\")\n",
    "    learning_rate_G = tf.placeholder(tf.float32, name=\"lr_g\")\n",
    "    learning_rate_D = tf.placeholder(tf.float32, name=\"lr_d\")\n",
    "    return inputs_real, inputs_z, learning_rate_G, learning_rate_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def show_samples(sample_images, name, epoch):\n",
    "    figure, axes = plt.subplots(1, len(sample_images), figsize = (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    for index, axis in enumerate(axes):\n",
    "        axis.axis('off')\n",
    "        image_array = sample_images[index]\n",
    "        axis.imshow(image_array)\n",
    "        image = Image.fromarray(image_array)\n",
    "        image.save(name+\"_\"+str(epoch)+\"_\"+str(index)+\".png\") \n",
    "    plt.savefig(name+\"_\"+str(epoch)+\".png\", bbox_inches='tight', pad_inches=0)\n",
    "    plt.show()\n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def test(sess, input_z, out_channel_dim, epoch):\n",
    "    example_z = np.random.uniform(-1, 1, size=[SAMPLES_TO_SHOW, input_z.get_shape().as_list()[-1]])\n",
    "    samples = sess.run(generator(input_z, out_channel_dim, False), feed_dict={input_z: example_z})\n",
    "    sample_images = [((sample + 1.0) * 127.5).astype(np.uint8) for sample in samples]\n",
    "    show_samples(sample_images, OUTPUT_DIR + \"samples\", epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def summarize_epoch(epoch, duration, sess, d_losses, g_losses, input_z, data_shape):\n",
    "    minibatch_size = int(data_shape[0]//BATCH_SIZE)\n",
    "    print(\"Epoch {}/{}\".format(epoch, EPOCHS),\n",
    "          \"\\nDuration: {:.5f}\".format(duration),\n",
    "          \"\\nD Loss: {:.5f}\".format(np.mean(d_losses[-minibatch_size:])),\n",
    "          \"\\nG Loss: {:.5f}\".format(np.mean(g_losses[-minibatch_size:])))\n",
    "    fig, ax = plt.subplots()\n",
    "    plt.plot(d_losses, label='Discriminator', alpha=0.6)\n",
    "    plt.plot(g_losses, label='Generator', alpha=0.6)\n",
    "    plt.title(\"Losses\")\n",
    "    plt.legend()\n",
    "    plt.savefig(OUTPUT_DIR + \"losses_\" + str(epoch) + \".png\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "    test(sess, input_z, data_shape[3], epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train(get_batches, data_shape, checkpoint_to_load=None):\n",
    "    input_images, input_z, lr_G, lr_D = model_inputs(data_shape[1:], NOISE_SIZE)\n",
    "    d_loss, g_loss = model_loss(input_images, input_z, data_shape[3])\n",
    "    d_opt, g_opt = model_optimizers(d_loss, g_loss)\n",
    "    config = tf.ConfigProto(device_count = {'GPU': 1})\n",
    "    with tf.Session(config=config) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        epoch = 0\n",
    "        iteration = 0\n",
    "        d_losses = []\n",
    "        g_losses = []\n",
    "        \n",
    "        for epoch in range(EPOCHS):        \n",
    "            epoch += 1\n",
    "            start_time = time.time()\n",
    "\n",
    "            for batch_images in get_batches:\n",
    "                iteration += 1\n",
    "                batch_z = np.random.uniform(-1, 1, size=(BATCH_SIZE, NOISE_SIZE))\n",
    "                _ = sess.run(d_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_D: LR_D})\n",
    "                _ = sess.run(g_opt, feed_dict={input_images: batch_images, input_z: batch_z, lr_G: LR_G})\n",
    "                d_losses.append(d_loss.eval({input_z: batch_z, input_images: batch_images}))\n",
    "                g_losses.append(g_loss.eval({input_z: batch_z}))\n",
    "\n",
    "            summarize_epoch(epoch, time.time()-start_time, sess, d_losses, g_losses, input_z, data_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "NOISE_SIZE = 100\n",
    "LR_D = 0.00004\n",
    "LR_G = 0.0004\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 50 # For better results increase this value \n",
    "BETA1 = 0.5\n",
    "WEIGHT_INIT_STDDEV = 0.02\n",
    "EPSILON = 0.00005\n",
    "SAMPLES_TO_SHOW = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Start!\")\n",
    "with tf.Graph().as_default():\n",
    "    train(get_batches(input_images), input_images.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
